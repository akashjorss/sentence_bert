{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorboard_projector_plugin.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.0 64-bit ('topic_modelling': conda)"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e94f718ea4c148639fa202da4c755c6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_22468ae5f29a4f09b5ff41505fecb3d6",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e593ee80eddf463f8b880191c1170653",
              "IPY_MODEL_7b43358fbb944178a57aea9942acf1b0"
            ]
          }
        },
        "22468ae5f29a4f09b5ff41505fecb3d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e593ee80eddf463f8b880191c1170653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e1710620037d49d0969c1063f3e4791c",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 403747457,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 403747457,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ddbdbc2352e48c0ab63e2c4d4a3a2ed"
          }
        },
        "7b43358fbb944178a57aea9942acf1b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8e595fdd197f40d3a3d93a1b9a1e2127",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 404M/404M [00:53&lt;00:00, 7.60MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f39d950825fd4bb782106cb9582fdfa6"
          }
        },
        "e1710620037d49d0969c1063f3e4791c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ddbdbc2352e48c0ab63e2c4d4a3a2ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e595fdd197f40d3a3d93a1b9a1e2127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f39d950825fd4bb782106cb9582fdfa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "interpreter": {
      "hash": "33ea4571294b160a8c9ba59d2d1217d533cade9e0aa979f130bb0787997e3281"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JXr1g7obLM4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "4945cd0a-89d0-4421-d108-2f1d58e803c6"
      },
      "source": [
        "#This notebook is for evaluating different topic models\n",
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: WANDB_MODE=offline\nenv: WANDB_NOTEBOOK_NAME='./comparison_different_TM.ipynb'\n"
          ]
        }
      ],
      "source": [
        "%set_env WANDB_MODE=offline\n",
        "%set_env WANDB_NOTEBOOK_NAME='./comparison_different_TM.ipynb'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_L0sV00ZLYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfc400b9-9bcf-48b5-bdff-e716692f4b23"
      },
      "source": [
        "#import required packages\n",
        "\n",
        "#typical imports\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json\n",
        "from multiprocessing import Pool\n",
        "from time import time\n",
        "\n",
        "#nltk imports\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#transformer imports\n",
        "from top2vec import Top2Vec\n",
        "import top2vec\n",
        "#miscellaneous\n",
        "import fasttext\n",
        "import wandb\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Finishing last run (ID:2bza8col) before initializing another..."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<br/>Waiting for W&B process to finish, PID 28917<br/>Program ended successfully."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find user logs for this run at: <code>/home/akash/topic_modelling/wandb/offline-run-20210617_214334-2bza8col/logs/debug.log</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find internal logs for this run at: <code>/home/akash/topic_modelling/wandb/offline-run-20210617_214334-2bza8col/logs/debug-internal.log</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mwandb sync /home/akash/topic_modelling/wandb/offline-run-20210617_214334-2bza8col\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "...Successfully finished last run (ID:2bza8col). Initializing new run:<br/><br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to `offline` in this directory.  Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": "<h1>Run(2rnak30p)</h1><iframe src=\"\" style=\"border:none;width:100%;height:400px\"></iframe>",
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f7ccbad8700>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#initialize W&B -> doesn't work because of problems in online connection. Maybe solving SSL certificate error will also resolve this. \n",
        "import wandb \n",
        "# wandb.login()\n",
        "wandb.init(\n",
        "  project='comparison_topic_modelling_booking_reviews',\n",
        "  config={\n",
        "      'dataset':'booking.com reviews (randomly sampled 100k)',\n",
        "      'top_k_words':20\n",
        "  }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "#load fastText embeddings\n",
        "ft_embeddings = fasttext.load_model('models/fast_text_embeddings.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load the models\n",
        "# doc2vec = Top2Vec.load('models/doc2vec_top2vec_without_preprocessing.bin')\n",
        "# use = Top2Vec.load('models/use_top2vec_without_preprocessing.bin')\n",
        "# sbert = Top2Vec.load('models/sbert_top2vec_without_preprocessing.bin')\n",
        "# tsdae = Top2Vec.load('models/tsdae_top2vec_without_preprocessing.bin')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aOZAQwvfO8Q",
        "outputId": "b604d4a9-fc28-4deb-8721-95504b2bb0ef"
      },
      "source": [
        "#Some statistics about topic models\n",
        "# print(\"Total num of topics in USE:\", use.get_num_topics())\n",
        "# print(\"Total num of topics in doc2vec:\", doc2vec.get_num_topics())\n",
        "# print(\"Total num of topics in sentence-bert:\", sbert.get_num_topics())\n",
        "# print(\"Total num of topics in TSDAE:\", tsdae.get_num_topics())"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"data/booking_sentences.txt\", 'r') as f:\n",
        "    documents = f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNSs7r_UETCH"
      },
      "source": [
        "#Function to calculate information gain of a dataset docs and top k representative words\n",
        "def calc_IG(words, docs):\n",
        "  \"\"\"calculate information gain for a topic in topic model containing words and documents\n",
        "  args:\n",
        "    words (list): list of top words in the topic. \n",
        "    docs (list): list of docs in the topic\n",
        "  returns:\n",
        "    ig (float): information gain of this topic with respect to total independence b/w words and docs\"\"\"\n",
        "  #sanity check\n",
        "  assert(len(words)!=0)\n",
        "  assert(len(docs)!=0)\n",
        "  words = list(set(words)) #drop the redundant words\n",
        "  #convert words to lower case\n",
        "  words = [word.lower() for word in words]\n",
        "  #tokenize the docs using nltk punkt\n",
        "  tokenized_docs = [word_tokenize(doc.lower()) for doc in docs]\n",
        "  #create inverted index between words and docs\n",
        "  inverted_index = np.zeros((len(words),len(docs)))\n",
        "  for i in range(len(words)):\n",
        "    for j in range(len(docs)):\n",
        "      inverted_index[i][j] = tokenized_docs[j].count(words[i])\n",
        "  # print(words)\n",
        "  # print(inverted_index)\n",
        "  ig = 0.0\n",
        "  P_d = 1/len(docs)\n",
        "  P_w = 1/len(words)\n",
        "  for i in range(len(words)):\n",
        "    for j in range(len(docs)):\n",
        "      if np.sum(inverted_index[i]) != 0:\n",
        "        P_dw = inverted_index[i][j]/np.sum(inverted_index[i]) #total number of documents in which w occurs\n",
        "        if P_dw != 0:\n",
        "          ig += P_dw*P_w*np.log2(P_dw/P_d)\n",
        "  \n",
        "  return ig     \n",
        "\n",
        "#test\n",
        "# doc1 = \"The big sharks of Belgium drink beer.\"\n",
        "# doc2 = \"Belgium has great beer. They drink beer all the time.\"\n",
        "# doc3 = \"They drink a lot of beer in Belgium\"\n",
        "# doc4 = \"cha cha\"\n",
        "# words = [\"so\", \"what\"]\n",
        "# docs = [doc1, doc2, doc3, doc4]\n",
        "# calc_IG(words, docs)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORwGQbt5hEF_"
      },
      "source": [
        "#Function to calculate total information gain for a given model\n",
        "def calculate_total_information_gain(model, k_words = 10):\n",
        "  \"\"\"This function calculates the total information gain of the model.\n",
        "  args:\n",
        "    model (Top2Vec): Top2Vec model used in encoding word and document vectors\n",
        "    n_topics(int): Number of topics to reduce the model to\n",
        "    k_words(int): To consider top k words (by distance) for each topic while calculating IG\n",
        "  returns:\n",
        "    total_IG (float): sum of information gain of all the clusters\"\"\" \n",
        "  \n",
        "  #get topic words and ids\n",
        "  topic_words, similarity_scores, topic_ids = model.get_topics(reduced=True)\n",
        "  \n",
        "  #Get the keywords of each topic\n",
        "  #create a dictionary with key as topic_id and values as list of document ids\n",
        "  # print(\"Building topic index for keywords...\")\n",
        "  topic_words_dict = {}\n",
        "  for i in topic_ids:\n",
        "    topic_words_dict[i] = topic_words[i][:k_words] #top k words\n",
        "\n",
        "  #Get the topic number of each document\n",
        "  #create a dictionary with key as topic_id and values as list of document ids\n",
        "  # print(\"Building topic index for documents...\")\n",
        "  topic_docs_dict = {}\n",
        "  for i in topic_ids: \n",
        "    topic_docs_dict[i] = []\n",
        "\n",
        "  for i in range(len(documents)):\n",
        "    topic_id, _, _, _ = model.get_documents_topics([i], reduced=True)\n",
        "    topic_docs_dict[topic_id[0]].append(i)\n",
        "\n",
        "  \n",
        "  #get documents and keywords of each cluster and calculate the information gain\n",
        "  # print(\"Calculating information gain...\")\n",
        "  total_ig = 0\n",
        "  for key in topic_ids:\n",
        "    words = topic_words_dict[key]\n",
        "    doc_ids = topic_docs_dict[key]\n",
        "    docs = np.take(documents, doc_ids)\n",
        "    topic_ig = calc_IG(words, docs)\n",
        "    # print(\"\\nInformation gain for topic\", key, \": \", topic_ig)\n",
        "    total_ig += topic_ig\n",
        "\n",
        "  return total_ig"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1M_QsJbFYYq"
      },
      "source": [
        "def cos_similarity(vector1, vector2):\n",
        "  \"\"\"This functions finds the similarity between doc1 and doc2 using cosine similarity. \n",
        "  args:\n",
        "    vector1(list-like): document embedding 1\n",
        "    vector2(list-like): document embedding 2\n",
        "  returns:\n",
        "    cos_sim(float): cosine similarity score between vector1 and vector2\"\"\"\n",
        "  cos_sim = np.dot(vector1, vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))\n",
        "  return cos_sim\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZQEtvBcLDub"
      },
      "source": [
        "def calculate_total_coherence(model, k_words = 20):\n",
        "  \"\"\"calculates total coherence for a model, based on sum of pairwise word similarity of top k words in different topics\"\"\" \n",
        "  def calculate_coherence(words):\n",
        "    \"\"\"calculates coherence for top-k words using fastText embeddings by summing up the pairwise cosine distance\n",
        "    args:\n",
        "      words (list-like): list of words\n",
        "    returns:\n",
        "      coherence_score (float)\"\"\"\n",
        "    #assert that there are no duplicate words in the list\n",
        "    num_words = len(words)\n",
        "    coherence_score = 0\n",
        "    for i in range(num_words):\n",
        "      for j in range(num_words):\n",
        "        if i != j:\n",
        "          vec1 = ft_embeddings[words[i]]\n",
        "          vec2 = ft_embeddings[words[j]]\n",
        "          if cos_similarity(vec1, vec2) != np.nan:\n",
        "            cosine_dist = cos_similarity(vec1, vec2) \n",
        "            coherence_score += cosine_dist\n",
        "    coherence_score = coherence_score/num_words\n",
        "    return coherence_score\n",
        "\n",
        "  #get top k words for each topic\n",
        "  #get topic words and ids\n",
        "  topic_words, similarity_scores, topic_ids = model.get_topics(reduced=True)\n",
        "  \n",
        "  #Get the keywords of each topic\n",
        "  #create a dictionary with key as topic_id and values as list of document ids\n",
        "  # print(\"Building topic index for keywords...\")\n",
        "  topic_words_dict = {}\n",
        "  for i in topic_ids:\n",
        "    topic_words_dict[i] = topic_words[i][:k_words] #top k words\n",
        "  \n",
        "  avg_coherence = 0\n",
        "  for topic_id in topic_ids:\n",
        "    avg_coherence += calculate_coherence(topic_words_dict[topic_id])\n",
        "  avg_coherence = avg_coherence/len(topic_ids)\n",
        "  \n",
        "  return avg_coherence\n",
        "  \n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "use = Top2Vec.load('models/use_top2vec.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(args):\n",
        "  model_path, n_topics, model_name = args[0], args[1], args[2]\n",
        "\n",
        "  #open the model from file. (We don't pass it as an arg to not overload the memory)\n",
        "  model = Top2Vec.load(model_path)\n",
        "\n",
        "  print(\"\\nReducing number of topics to\", n_topics, \"...\")\n",
        "  model.hierarchical_topic_reduction(n_topics)\n",
        "    \n",
        "  print(f\"calculating IG for {model_name} for {n_topics} topics...\")\n",
        "  info_gain = calculate_total_information_gain(model, k_words=20)\n",
        "    \n",
        "  print(f\"calculating coherence for {model_name} for {n_topics} topics...\")\n",
        "  coherence = calculate_total_coherence(model, k_words=20)\n",
        "\n",
        "  wandb.log({'num_topics':n_topics, model_name+'_ig':info_gain, model_name+'_coherence':coherence})\n",
        "\n",
        "  run = []\n",
        "  run.append({'metric':'IG', 'model':model_name, 'num_topics':n_topics, 'top_k_words':20, 'score':info_gain})\n",
        "  run.append({'metric':'coherence', 'model':model_name, 'num_topics':n_topics, 'top_k_words':20, 'score':coherence})\n",
        "\n",
        "  with open(\"comparison_different_topic_models/run3.txt\", 'a') as f:\n",
        "    json.dump(run, f)\n",
        "\n",
        "  #free up the memory\n",
        "  del model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "A_hBfTBSvi_4",
        "outputId": "8f3a9326-3532-4fbf-abe1-1b83d787ed77"
      },
      "source": [
        "#find total information gain and total coherence\n",
        "\n",
        "models = {'doc2vec':'./models/doc2vec_top2vec_without_preprocessing.bin', \n",
        "          'use':'./models/use_top2vec_without_preprocessing.bin', \n",
        "          'sbert':'./models/sbert_top2vec_without_preprocessing.bin', \n",
        "          'tsdae':'./models/tsdae_top2vec_without_preprocessing.bin'}\n",
        "\n",
        "num_topics = list(range(10,11,10))\n",
        "#create args\n",
        "args = []\n",
        "for key in models:\n",
        "  for i in num_topics:\n",
        "    args.append([models[key], i, key])\n",
        "start = time()\n",
        "pool = Pool()\n",
        "pool.map(evaluate_model, args)\n",
        "stop = time()\n",
        "print(f\"That took {stop-start} seconds.\")\n",
        "\n",
        "wandb.finish()\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reducing number of topics to 10 ...\n",
            "\n",
            "Reducing number of topics to 10 ...\n",
            "\n",
            "Reducing number of topics to 10 ...\n",
            "\n",
            "Reducing number of topics to 10 ...\n",
            "calculating IG for use for 10 topics...\n",
            "calculating coherence for use for 10 topics...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m log() ignored (called from pid=29051, init called from pid=28139). See: https://docs.wandb.ai/library/init#multiprocess\n",
            "calculating IG for doc2vec for 10 topics...\n",
            "calculating coherence for doc2vec for 10 topics...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m log() ignored (called from pid=29050, init called from pid=28139). See: https://docs.wandb.ai/library/init#multiprocess\n",
            "calculating IG for sbert for 10 topics...\n",
            "calculating IG for tsdae for 10 topics...\n",
            "calculating coherence for sbert for 10 topics...\n",
            "<ipython-input-15-a8c3f7a225bf>:8: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  cos_sim = np.dot(vector1, vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m log() ignored (called from pid=29052, init called from pid=28139). See: https://docs.wandb.ai/library/init#multiprocess\n",
            "calculating coherence for tsdae for 10 topics...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m log() ignored (called from pid=29053, init called from pid=28139). See: https://docs.wandb.ai/library/init#multiprocess\n",
            "That took 6181.319442987442 seconds.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<br/>Waiting for W&B process to finish, PID 28979<br/>Program ended successfully."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find user logs for this run at: <code>/home/akash/topic_modelling/wandb/offline-run-20210617_214416-2rnak30p/logs/debug.log</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Find internal logs for this run at: <code>/home/akash/topic_modelling/wandb/offline-run-20210617_214416-2rnak30p/logs/debug-internal.log</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mwandb sync /home/akash/topic_modelling/wandb/offline-run-20210617_214416-2rnak30p\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "0emWuBS0Jpvp",
        "outputId": "51034316-6aa2-4bce-d949-9b29816ae551"
      },
      "source": [
        "#plot results using matplotlib\n",
        "df = pd.DataFrame(runs)\n",
        "df.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKsHndpWepGN"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Im2_KeMvtFsR",
        "outputId": "96a0b0c6-777c-4181-ece7-91dd0f028fa9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x,y_use[:8], label=\"use\")\n",
        "plt.plot(x,y_sbert, label=\"sbert\")\n",
        "# plt.plot(x, y_doc2vec, label=\"doc2vec\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Number of topics\")\n",
        "plt.ylabel(\"Average Topic Coherence\")\n",
        "plt.title(\"Coherence score for Top 20 words\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv6YBP8dIhIS"
      },
      "source": [
        "# def coherence(doc_vectors):\n",
        "#   \"\"\"This function calculates the topic coherence by taking pairwise cosine similarity\n",
        "#   args:\n",
        "#     doc_vectors(list-like): list of document vectors(512 or 768 dim)\n",
        "#   returns:\n",
        "#     coherence_score(float)\"\"\"\n",
        "#   num_docs = len(doc_vectors)\n",
        "#   coherence_score = 0\n",
        "#   similarity_matrix = np.zeros((num_docs, num_docs))\n",
        "#   for i in range(num_docs):\n",
        "#     for j in range(num_docs):\n",
        "#       if i!=j: #the similarity of doc to itself is 1 but we don't count it, so in this matrix it is 0\n",
        "#         coherence_score += cos_similarity(doc_vectors[i], doc_vectors[j])\n",
        "\n",
        "#   return coherence_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#upload to W&B\n",
        "#Set up weights and biases to visualize the training of TSDAE\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "  project='comparison_of_different_TM',\n",
        "  config={\n",
        "      'dataset':'booking.com reviews (randomly sampled 100k)',\n",
        "  }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logs = []\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(logs)\n",
        "print(df.head())\n",
        "logs = {}\n",
        "logs['doc2vec_ig'] = df[df['metric']=='IG'][df['model']=='doc2vec'].score.values\n",
        "logs['use_ig'] = df[df['metric']=='IG'][df['model']=='use'].score.values\n",
        "logs['tsdae_ig'] = df[df['metric']=='IG'][df['model']=='tsdae'].score.values\n",
        "logs['sbert_ig'] =  df[df['metric']=='IG'][df['model']=='sbert'].score.values\n",
        "\n",
        "logs['doc2vec_coherence'] = df[df['metric']=='coherence'][df['model']=='doc2vec'].score.values\n",
        "logs['use_coherence'] = df[df['metric']=='coherence'][df['model']=='use'].score.values\n",
        "logs['tsdae_coherence'] = df[df['metric']=='coherence'][df['model']=='tsdae'].score.values\n",
        "logs['sbert_coherence'] = df[df['metric']=='coherence'][df['model']=='sbert'].score.values\n",
        "\n",
        "for key in logs:\n",
        "  num_topics = 10\n",
        "  for val in logs[key]:\n",
        "    wandb.log({key: val, 'num_steps':num_topics})\n",
        "    num_topics +=10\n",
        "wandb.finish()"
      ]
    }
  ]
}
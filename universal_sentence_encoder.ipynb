{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorboard_projector_plugin.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JXr1g7obLM4"
      },
      "source": [
        "#different versio of numpy required for top2vec\n",
        "!pip uninstall numpy\n",
        "!pip install numpy==1.19.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaOLePJuGjO_"
      },
      "source": [
        "#install the packages\n",
        "# !pip install sentence-transformers\n",
        "# !pip install nltk\n",
        "# !pip install torch\n",
        "# !pip install tensorflow\n",
        "# !pip install tensorboard\n",
        "!pip install top2vec\n",
        "!pip install top2vec[sentence_encoders]\n",
        "# !pip install top2vec[sentence_transformers]\n",
        "# !pip install top2vec[indexing]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_L0sV00ZLYi"
      },
      "source": [
        "#import required packages\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olyW_M3saRRW"
      },
      "source": [
        "!git clone https://github.com/akashjorss/sentence_bert\n",
        "!unzip sentence_bert/sentences.zip\n",
        "import pickle\n",
        "with open('sentences.bin', 'rb') as f:\n",
        "  sentences = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0on38X_aUTh"
      },
      "source": [
        "# import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorboard.plugins import projector\n",
        "#load tensorboards with magics\n",
        "%tensorflow_version 2.x\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNQdC9K5aZ8D"
      },
      "source": [
        "documents = random.choices(sentences,k = 100000)\n",
        "documents = [d.lower() for d in documents]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ3yN_mdZY9G"
      },
      "source": [
        "from top2vec import Top2Vec\n",
        "model = Top2Vec(documents, embedding_model='universal-sentence-encoder', workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQJycY3TcfF-"
      },
      "source": [
        "model.hierarchical_topic_reduction(100)\n",
        "# model.get_topic_hierarchy()\n",
        "# topic_sizes, topic_nums = model.get_topic_sizes()\n",
        "print(\"Original Number of topics: \", model.get_num_topics())\n",
        "print(\"Reduced number of topics: \", model.get_num_topics(reduced=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKsHndpWepGN"
      },
      "source": [
        "#word similarity\n",
        "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=[\"swimming\"], num_topics=3, reduced=False) #swimming, cleanliness #keywords_neg=stop,\n",
        "for topic in topic_nums:\n",
        "    model.generate_topic_wordcloud(topic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQzZ6eRkKz7I"
      },
      "source": [
        "#reduce the number of topics and do hierarchical clustering\n",
        "# topic_words, word_scores, topic_nums = model.get_topics(reduced=True)\n",
        "# topic_nums = random.choices(topic_nums, k=20)\n",
        "# for topic in topic_nums:\n",
        "#     model.generate_topic_wordcloud(topic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsyVBhzyPd_U"
      },
      "source": [
        "#get document vectors and plot using umap and tensorboard\n",
        "#dim reduction using umap\n",
        "import umap\n",
        "umap_embeddings = umap.UMAP(n_neighbors=15,\n",
        "                            n_components=3, \n",
        "                            metric='cosine').fit_transform(model.document_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sev-FR1-P2o7"
      },
      "source": [
        "#visualize high dim data in embedding projector\n",
        "# Set up a logs directory, so Tensorboard knows where to look for files\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "log_dir='./logs/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# Save the weights we want to analyse as a variable. \n",
        "umap_weights = tf.Variable(random.choices(umap_embeddings, k=10000))\n",
        "# Create a checkpoint from embedding, the filename and key are\n",
        "# name of the tensor.\n",
        "checkpoint = tf.train.Checkpoint(embedding=umap_weights)\n",
        "checkpoint.save(os.path.join(log_dir, \"umap_embedding.ckpt\"))\n",
        "\n",
        "# Set up config\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`\n",
        "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
        "# embedding.metadata_path = 'metadata.tsv'\n",
        "projector.visualize_embeddings(log_dir, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOmThAX5mNle"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bg-_0DWQEto"
      },
      "source": [
        "%tensorboard --logdir ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOxfH6KzYvdb"
      },
      "source": [
        "#visualize word embeddings: get rid of stopwords, sentiment words, and take more sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuKztLMlh8jR"
      },
      "source": [
        "#function to process sentence to remove stopwords and sentiment words\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "def process_sentence(s):\n",
        "  s = s.lower()\n",
        "\n",
        "  stop = stopwords.words('english')\n",
        "  sentiment_tags = ['ADJ', 'ADV', 'RB', 'VBZ']\n",
        "\n",
        "\n",
        "  words = word_tokenize(s)\n",
        "  #remove stop words\n",
        "  words = [w for w in words if w not in stop]\n",
        "  #remove sentiment words\n",
        "  pos_tags = nltk.pos_tag(words)\n",
        "  words = [t[0] for t in pos_tags if t[1] not in sentiment_tags]\n",
        "  s = \" \".join(words)\n",
        "  return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0Hrrn0ZdlNL"
      },
      "source": [
        "# random.choices(documents, k=10)\n",
        "#sentence similarity\n",
        "# s = \"oh and it is an additional dollars per night to park your car there!\"\n",
        "# s = process_sentence(s)\n",
        "# #process s to remove stopwords and sentiment words\n",
        "\n",
        "# topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=s.split(), num_topics=3, reduced=False) #swimming, cleanliness #keywords_neg=stop,\n",
        "# for topic in topic_nums:\n",
        "#     model.generate_topic_wordcloud(topic)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
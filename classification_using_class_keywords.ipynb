{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('topic_modelling': conda)"
  },
  "interpreter": {
   "hash": "33ea4571294b160a8c9ba59d2d1217d533cade9e0aa979f130bb0787997e3281"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simple classification using sentence embedding models. Following are the steps:\n",
    "1. Load a model (e.g. doc2vec)\n",
    "2. Load keywords for each class\n",
    "3. measure distance between each class and a sentence. Select argmin. \"\"\" \n",
    "#imports\n",
    "from absl import logging\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import sentencepiece as spm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-lite/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_placeholder = tf.sparse_placeholder(tf.int64, shape=[None, None])\n",
    "encodings = module(\n",
    "    inputs=dict(\n",
    "        values=input_placeholder.values,\n",
    "        indices=input_placeholder.indices,\n",
    "        dense_shape=input_placeholder.dense_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "  spm_path = sess.run(module(signature=\"spm_path\"))\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "with tf.io.gfile.GFile(spm_path, mode=\"rb\") as f:\n",
    "  sp.LoadFromSerializedProto(f.read())\n",
    "print(\"SentencePiece model loaded at {}.\".format(spm_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_to_IDs_in_sparse_format(sp, sentences):\n",
    "  # An utility method that processes sentences with the sentence piece processor\n",
    "  # 'sp' and returns the results in tf.SparseTensor-similar format:\n",
    "  # (values, indices, dense_shape)\n",
    "  ids = [sp.EncodeAsIds(x) for x in sentences]\n",
    "  max_len = max(len(x) for x in ids)\n",
    "  dense_shape=(len(ids), max_len)\n",
    "  values=[item for sublist in ids for item in sublist]\n",
    "  indices=[[row,col] for row in range(len(ids)) for col in range(len(ids[row]))]\n",
    "  return (values, indices, dense_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a representation for each message, showing various lengths supported.\n",
    "word = \"Elephant\"\n",
    "sentence = \"I am a sentence for which I would like to get its embedding.\"\n",
    "paragraph = (\n",
    "    \"Universal Sentence Encoder embeddings also support short paragraphs. \"\n",
    "    \"There is no hard limit on how long the paragraph is. Roughly, the longer \"\n",
    "    \"the more 'diluted' the embedding will be.\")\n",
    "messages = [word, sentence, paragraph]\n",
    "\n",
    "values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, messages)\n",
    "\n",
    "# Reduce logging output.\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "with tf.Session() as session:\n",
    "  session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "  message_embeddings = session.run(\n",
    "      encodings,\n",
    "      feed_dict={input_placeholder.values: values,\n",
    "                input_placeholder.indices: indices,\n",
    "                input_placeholder.dense_shape: dense_shape})\n",
    "\n",
    "  for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "    print(\"Message: {}\".format(messages[i]))\n",
    "    print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "    message_embedding_snippet = \", \".join(\n",
    "        (str(x) for x in message_embedding[:3]))\n",
    "    print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(messages):\n",
    "    \"\"\"uses USE to get embeddings of messages.\n",
    "    message is array of strings. String could be a word or a sentence.\"\"\"\n",
    "    values, indices, dense_shape = process_to_IDs_in_sparse_format(sp, messages)\n",
    "    message_embeddings = None\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(\n",
    "        encodings,\n",
    "        feed_dict=\n",
    "        {input_placeholder.values: values,\n",
    "                    input_placeholder.indices: indices,\n",
    "                    input_placeholder.dense_shape: dense_shape})\n",
    "    return np.array(message_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings(['clean', 'dirty'])\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_class_keywords():\n",
    "    \"\"\"generates class keywords and class embeddings\"\"\"\n",
    "    #keywords could be generated automatically using word_embedding models\n",
    "    keywords = {'staff':['staff', 'reception', 'waiter', 'housekeeping'], \n",
    "                'location':['location', 'area', 'neighborhood', 'neighbourhood', 'near', 'far'], \n",
    "                'service':['service', ' helpful', ' facility'], #redundant with staff\n",
    "                'room':['room', 'bedroom', 'bed', 'floor'], \n",
    "                'sleep_quality':['sleep quality', 'sleep', 'insomnia', 'noisy', 'noise', 'bed',                                         'pillows'], \n",
    "                'swimming_pool':['swimming', 'pool', 'jacuzzi', 'pools'], \n",
    "                'value_for_money':['expensive', 'cheap', 'cost', 'price', 'value for money'], \n",
    "                'cleanliness':['cleanliness', 'clean', 'bathroom', 'toilet', 'dirty', 'spotless',                                       'sanitary', 'unclean', 'tidy']}\n",
    "\n",
    "    embeddings = {}\n",
    "    for key in keywords:\n",
    "        embeddings[key] = get_embeddings(keywords[key])\n",
    "    \n",
    "    return keywords, embeddings\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_keywords, class_embeddings = create_class_keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(vector1, vector2):\n",
    "  cos_sim = np.dot(vector1, vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))\n",
    "  return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def review_aspects(review, class_embeddings):\n",
    "    #sentence tokenization\n",
    "    sentences = nltk.tokenize.sent_tokenize(review)\n",
    "    \n",
    "    sentence_embeddings = get_embeddings(sentences)\n",
    "    sentence_class = {}\n",
    "    for i in range(len(sentences)):\n",
    "        class_distances = {}\n",
    "        for key in class_embeddings:\n",
    "            min_dist = np.Infinity\n",
    "            for keyword_embed in class_embeddings[key]:\n",
    "                dist=abs(cos_similarity(keyword_embed, sentence_embeddings[i]))\n",
    "                if min_dist > dist:\n",
    "                    min_dist = dist\n",
    "            class_distances[key] = min_dist\n",
    "\n",
    "        #the right approach is actually outlier detection, use 3 sigma rule\n",
    "        #mu  = mean of the data\n",
    "        #std = standard deviation of the data\n",
    "        #IF abs(x-mu) > 3*std  THEN  x is outlier\n",
    "        mu = np.mean(list(class_distances.values()))\n",
    "        print(list(class_distances.values()))\n",
    "        # print('mu', mu)\n",
    "        std = np.std(list(class_distances.values()))\n",
    "        sentence_class[i] = None\n",
    "        for key in class_distances:\n",
    "            x = class_distances[key]\n",
    "            if x-mu < 0:\n",
    "                if abs(x-mu) > 2*std:\n",
    "                    sentence_class[i] = key\n",
    "                    break #wrong logic, should be minimum of whichever options pass the outlier test\n",
    "    return sentences, sentence_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.16856785, 0.18015818, 0.14343667, 0.15319811, 0.061022647, 0.12810887, 0.10749574, 0.1624117]\n[0.21744406, 0.18552805, 0.123958535, 0.07767787, 0.032654315, 0.03220264, 0.17714652, 0.067323595]\n[0.09785338, 0.062012054, 0.048544865, 0.06243664, 0.04928684, 0.07428492, 0.08454848, 0.094223544]\n[0.13947722, 0.0406598, 0.043107465, 0.066168755, 0.016844431, 0.0701942, 0.018802041, 0.020055488]\n[0.10922802, 0.05728433, 0.13644898, 0.10538236, 0.06376191, 0.029890656, 0.13340549, 0.049658675]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['Stumbled across this restaurant while walking near Antibes.',\n",
       "  'Was initially quiet but steady stream of customers created a friendly atmosphere.',\n",
       "  'I ordered steak, which was perfect and the same dessert twice as it was to die for.',\n",
       "  'The staff were fantastic and made the evening great and it didn’t feel like I was dining alone.',\n",
       "  'Thanks Matthieu.'],\n",
       " {0: 'sleep_quality', 1: None, 2: None, 3: None, 4: None})"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "review= \"Stumbled across this restaurant while walking near Antibes. Was initially quiet but steady stream of customers created a friendly atmosphere. I ordered steak, which was perfect and the same dessert twice as it was to die for. The staff were fantastic and made the evening great and it didn’t feel like I was dining alone. Thanks Matthieu.\" \n",
    "review_aspects(review, class_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A suggestion:\n",
    "#1. Train fasttext embeddings on my data.\n",
    "#2. Preprocess the data so that only noun phrases are remaining. \n",
    "#3. Compare the distance of class keywords to the noun phrases in the sentence\n",
    "#4. Use the same logic as above to select the class "
   ]
  }
 ]
}
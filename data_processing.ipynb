{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 64-bit ('topic_modelling': conda)"
  },
  "interpreter": {
   "hash": "33ea4571294b160a8c9ba59d2d1217d533cade9e0aa979f130bb0787997e3281"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'The goal of this notebook is to process the booking.com reviews to the desired format. It includes:\\n1. Tokenising reviews to sentences. \\n2. Removing Named entities from the sentences. \\n3. Removing adjectives, adverbs, and other sentiment words from sentences. \\n4. Removing stop words from sentences. \\nThis is only for the english data.'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "\"\"\"The goal of this notebook is to process the booking.com reviews to the desired format. It includes:\n",
    "1. Tokenising reviews to sentences. \n",
    "2. Removing Named entities from the sentences. \n",
    "3. Removing adjectives, adverbs, and other sentiment words from sentences. \n",
    "4. Removing stop words from sentences. \n",
    "This is only for the english data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import string\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def text_processing_pipeline(text):\n",
    "    \"\"\"text is list of texts\"\"\"\n",
    "\n",
    "    #sentence tokenization\n",
    "    # sentences = []\n",
    "    # print(\"sentence tokenization...\")\n",
    "    # for r in tqdm(text):\n",
    "    #     sentences.extend(nltk.tokenize.sent_tokenize(r))\n",
    "\n",
    "    sentences = text\n",
    "\n",
    "    #NER\n",
    "    print(\"named entity removal...\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for i in tqdm(range(len(sentences))):\n",
    "        s = sentences[i]\n",
    "        doc = nlp(s)\n",
    "        named_entities = [ent.text for ent in doc.ents]\n",
    "        s = nltk.word_tokenize(s)\n",
    "        s = [word for word in s if word not in named_entities]\n",
    "        sentences[i] = \" \".join(s)\n",
    "    \n",
    "    #sentiment words removal\n",
    "    print(\"Removal of adjective words...\")\n",
    "    sentiment_tags = ['JJ','JJR','JJS', 'RBR' ,'RBS', 'CD']\n",
    "    for i in tqdm(range(len(sentences))):\n",
    "        s = sentences[i].split()\n",
    "        pos_tagged = nltk.pos_tag(s)\n",
    "        sentiment_words = [t[0] for t in pos_tagged if t[1] in sentiment_tags]\n",
    "        s = [word for word in s if word not in sentiment_words]\n",
    "        sentences[i] = \" \".join(s)\n",
    "\n",
    "    #case desensitise\n",
    "    sentences = [s.lower() for s in sentences]\n",
    "\n",
    "    #lemmatization/stemming\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"lemmatization...\")\n",
    "    for i in tqdm(range(len(sentences))):\n",
    "        s = nlp(sentences[i])\n",
    "        lemmatised_s = []\n",
    "        for token in s:\n",
    "            lemmatised_s.append(token.lemma_)\n",
    "        sentences[i] = \" \".join(lemmatised_s)\n",
    "\n",
    "    #get list of punctuations and remove the word if it is a punctuation\n",
    "    print(\"punctuation_removal...\")\n",
    "    punctuations = string.punctuation\n",
    "    for i in tqdm(range(len(sentences))):\n",
    "        s = sentences[i]\n",
    "        tokens = s.split()\n",
    "        tokens = [t for t in tokens if t not in punctuations]\n",
    "        sentences[i] = \" \".join(tokens)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 100000/100000 [00:27<00:00, 3624.73it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/booking_reviews.csv')\n",
    "df = df.fillna(\"\")\n",
    "#get the reviews\n",
    "reviews = []\n",
    "for i in tqdm(range(len(df))):\n",
    "  neg_review = df.iloc[i].neg_review\n",
    "  pos_review = df.iloc[i].pos_review\n",
    "  if neg_review != \"\" and pos_review!= \"\":\n",
    "    reviews.append(neg_review + \" \" + pos_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 42065/42065 [00:04<00:00, 9556.90it/s]\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "sentences = []\n",
    "for r in tqdm(reviews):\n",
    "    sentences.extend(nltk.tokenize.sent_tokenize(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save sentences with pickle\n",
    "with open(\"data/booking_sentences.txt\", 'w') as f:\n",
    "    for s in sentences:\n",
    "        f.writelines(s)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
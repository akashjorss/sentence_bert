{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python380jvsc74a57bd033ea4571294b160a8c9ba59d2d1217d533cade9e0aa979f130bb0787997e3281",
   "display_name": "Python 3.8.0 64-bit ('topic_modelling': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The goal of this notebook is to process the booking.com reviews to the desired format. It includes:\n",
    "1. Tokenising reviews to sentences. \n",
    "2. Removing Named entities from the sentences. \n",
    "3. Removing adjectives, adverbs, and other sentiment words from sentences. \n",
    "4. Removing stop words from sentences. \n",
    "This is only for the english data.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install desired packages\n",
    "# !pip install nltk -q\n",
    "# !pip install spacy -q\n",
    "# !pip install numpy -q\n",
    "# !pip install pandas -q\n",
    "# nltk.download('all')\n",
    "# spacy.load('en_core_web_sm')\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 100000/100000 [00:25<00:00, 3848.26it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./booking_reviews.csv')\n",
    "df = df.fillna(\"\")\n",
    "#get the reviews\n",
    "reviews = []\n",
    "for i in tqdm(range(len(df))):\n",
    "  neg_review = df.iloc[i].neg_review\n",
    "  pos_review = df.iloc[i].pos_review\n",
    "  if neg_review != \"\" and pos_review!= \"\":\n",
    "    reviews.append(neg_review + \" \" + pos_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"I think breakfast could be improved. I have never stayed before at Hotel Hetman and I must admit it was a great discovery.  The hotel is located in the Praga neighborhood of Warsaw, which was once run down, but now is enjoying a renaissance, fueled by the new metro line; the Wilenska stop which is literally right next to the hotel.  Another option of traveling to central Warsaw is by tram, which is also conveniently close.  Another advantage of Hetman is the proximity to Le Cedre, Warsaw's leading Lebanese restaurant for over 20 years.\",\n",
       " 'Some cleaning issues but these were resolved immediately. There were a few maintenance issues. Great views, comfy beds, lovely hot tub.',\n",
       " \"Breakfast was a bit disappointing, as there wasn't much choice. Probably because of the weird times we are living now, but still other options could have been offered The staff is really friendly and gives you tips about the island.Our room was amazing, with a great view!The location is also good, as you are just 10 minutes away from the center.\",\n",
       " 'Steel bulk bads are creakly Very pleasant staffTasty dinner for 10eurNice surrounding around hostel',\n",
       " 'Nothing. Friendly staff, they let us check in early. I’m pleased we chose the Marina side as it is sheltered (and believe me, it can get windy!) as the other side catches the wind and shade!',\n",
       " 'Room was hot barely in the to hot to be in really would like to be completely move to a new room for free There was nothing to like',\n",
       " 'All good Beautiful place, beds were great',\n",
       " 'Stated in the facility details they provide microwave/oven, but they dont have it. The stove is not working. Does not refill the shampoo, body shower.  Big space room',\n",
       " 'The only negative thing was the hand basin which due to its shape and size made it difficult to wash your face. Having said that we understand that given the space available it has to be as it is. Jane & Charlie were so welcoming and friendly. The breakfasts were great, good choice of cereals and hot food. Plentiful supply of tea and toast. Room was very clean and well presented. Tea, milk and biscuits restocked in room every day.',\n",
       " 'Nothing Perfect location. I loved how it was in the city centre and central for everything']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "reviews[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 42065/42065 [00:04<00:00, 9998.06it/s] \n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "sentences = []\n",
    "for r in tqdm(reviews):\n",
    "    sentences.extend(nltk.tokenize.sent_tokenize(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 148114/148114 [16:14<00:00, 152.04it/s]\n"
     ]
    }
   ],
   "source": [
    "#named entity removal using spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for i in tqdm(range(len(sentences))):\n",
    "  s = sentences[i]\n",
    "  doc = nlp(s)\n",
    "  named_entities = [ent.text for ent in doc.ents]\n",
    "  s = nltk.word_tokenize(s)\n",
    "  s = [word for word in s if word not in named_entities]\n",
    "  sentences[i] = \" \".join(s)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment words removal\n",
    "sentiment_tags = ['JJ','JJR','JJS', 'RBR' ,'RBS', 'CD']\n",
    "for i in tqdm(range(len(sentences))):\n",
    "    s = sentences[i].split()\n",
    "    pos_tagged = nltk.pos_tag(s)\n",
    "    sentiment_words = [t[0] for t in pos_tagged if t[1] in sentiment_tags]\n",
    "    s = [word for word in s if word not in sentiment_words]\n",
    "    sentences[i] = \" \".join(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop word removal -> technically in top2vec, no need to remove\n",
    "# stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "# for i in tqdm(range(len(sentences))):\n",
    "#   s = sentences[i].split()\n",
    "#   s = [word for word in s if word not in stopwords]\n",
    "#   sentences[i] = \" \".join(s)\n",
    "sentences = [s.lower() for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 148114/148114 [18:16<00:00, 135.12it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'I ’ m pleased we choose the side as it be shelter ( and believe I , it can get ! )'"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "#lemmatization/stemming\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for i in tqdm(range(len(sentences))):\n",
    "    s = nlp(sentences[i])\n",
    "    lemmatised_s = []\n",
    "    for token in s:\n",
    "        lemmatised_s.append(token.lemma_)\n",
    "    sentences[i] = \" \".join(lemmatised_s)\n",
    "#test\n",
    "sentences[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 148114/148114 [00:00<00:00, 200088.94it/s]\n"
     ]
    }
   ],
   "source": [
    "#get list of punctuations and remove the word if it is a punctuation\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "for i in tqdm(range(len(sentences))):\n",
    "    s = sentences[i]\n",
    "    tokens = s.split()\n",
    "    tokens = [t for t in tokens if t not in punctuations]\n",
    "    sentences[i] = \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'same of the staff be lack training'"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "#Spell Correction\n",
    "#Since training our own model, no need to do spell correction\n",
    "#But there should be some cut off.  \n",
    "sentences[1000]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "148114"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save sentences with pickle\n",
    "with open(\"booking_sentences_processed.txt\", 'w') as f:\n",
    "    for s in sentences:\n",
    "        f.writelines(s)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing_pipeline(text):\n",
    "    \"\"\"text is list of texts\"\"\"\n",
    "\n",
    "    #sentence tokenization\n",
    "    # sentences = []\n",
    "    # print(\"sentence tokenization...\")\n",
    "    # for r in tqdm(text):\n",
    "    #     sentences.extend(nltk.tokenize.sent_tokenize(r))\n",
    "\n",
    "    sentences = text\n",
    "\n",
    "    #NER\n",
    "    print(\"named entity removal...\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    for i in tqdm(range(len(sentences))):\n",
    "        s = sentences[i]\n",
    "        doc = nlp(s)\n",
    "        named_entities = [ent.text for ent in doc.ents]\n",
    "        s = nltk.word_tokenize(s)\n",
    "        s = [word for word in s if word not in named_entities]\n",
    "        sentences[i] = \" \".join(s)\n",
    "    \n",
    "    #sentiment words removal\n",
    "    print(\"Removal of adjective words...\")\n",
    "    sentiment_tags = ['JJ','JJR','JJS', 'RBR' ,'RBS', 'CD']\n",
    "    for i in tqdm(range(len(sentences))):\n",
    "        s = sentences[i].split()\n",
    "        pos_tagged = nltk.pos_tag(s)\n",
    "        sentiment_words = [t[0] for t in pos_tagged if t[1] in sentiment_tags]\n",
    "        s = [word for word in s if word not in sentiment_words]\n",
    "        sentences[i] = \" \".join(s)\n",
    "\n",
    "    #case desensitise\n",
    "    sentences = [s.lower() for s in sentences]\n",
    "\n",
    "    #lemmatization/stemming\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"lemmatization...\")\n",
    "    for i in tqdm(range(len(sentences))):\n",
    "        s = nlp(sentences[i])\n",
    "        lemmatised_s = []\n",
    "        for token in s:\n",
    "            lemmatised_s.append(token.lemma_)\n",
    "        sentences[i] = \" \".join(lemmatised_s)\n",
    "\n",
    "    #get list of punctuations and remove the word if it is a punctuation\n",
    "    print(\"punctuation_removal...\")\n",
    "    punctuations = string.punctuation\n",
    "    for i in tqdm(range(len(sentences))):\n",
    "        s = sentences[i]\n",
    "        tokens = s.split()\n",
    "        tokens = [t for t in tokens if t not in punctuations]\n",
    "        sentences[i] = \" \".join(tokens)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load sts dataset for evaluation\n",
    "with open('sts-mt.csv', 'r') as f:\n",
    "  sts_lines = f.readlines() \n",
    "\n",
    "sts_df = []\n",
    "for l in sts_lines:\n",
    "  sts_df.append(l[:-1].split('\\t'))\n",
    "\n",
    "sentences1, sentences2, scores = [], [], []\n",
    "for l in sts_df:\n",
    "  scores.append(l[3])\n",
    "  sentences1.append(l[4])\n",
    "  sentences2.append(l[5])\n",
    "#scores are from 0 to 5. So the following line normalizes the score\n",
    "scores = [float(s)/5.0 for s in scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "named entity removal...\n",
      "100%|██████████| 1836/1836 [00:31<00:00, 58.77it/s]\n",
      "  3%|▎         | 61/1836 [00:00<00:02, 608.32it/s]Removal of adjective words...\n",
      "100%|██████████| 1836/1836 [00:03<00:00, 488.05it/s]\n",
      "  0%|          | 8/1836 [00:00<00:26, 67.80it/s]lemmatization...\n",
      "100%|██████████| 1836/1836 [00:29<00:00, 61.51it/s]\n",
      "100%|██████████| 1836/1836 [00:00<00:00, 97134.70it/s]\n",
      "punctuation_removal...\n",
      "named entity removal...\n",
      "100%|██████████| 1836/1836 [00:34<00:00, 52.79it/s]\n",
      "  4%|▎         | 66/1836 [00:00<00:02, 655.12it/s]Removal of adjective words...\n",
      "100%|██████████| 1836/1836 [00:04<00:00, 439.87it/s]\n",
      "  0%|          | 6/1836 [00:00<00:34, 52.64it/s]lemmatization...\n",
      "100%|██████████| 1836/1836 [00:30<00:00, 61.06it/s]\n",
      "100%|██████████| 1836/1836 [00:00<00:00, 90952.22it/s]punctuation_removal...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences1 = text_processing_pipeline(sentences1)\n",
    "sentences2 = text_processing_pipeline(sentences2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the files\n",
    "# os.mkdir('sts_processed')\n",
    "\n",
    "with open('sts_processed/sentences1.txt', 'w') as f:\n",
    "    for s in sentences1:\n",
    "        f.write(s)\n",
    "        f.write('\\n')\n",
    "with open('sts_processed/sentences2.txt', 'w') as f:\n",
    "    for s in sentences2:\n",
    "        f.write(s)\n",
    "        f.write('\\n')\n",
    "with open('sts_processed/scores.txt', 'w') as f:\n",
    "    for score in scores:\n",
    "        f.write(str(score))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1836"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "with open('sts_processed/sentences1.txt', 'r') as f:\n",
    "    temp = f.readlines()\n",
    "len(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
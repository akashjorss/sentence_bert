{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "train_models.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEn2wEqACaXj"
      },
      "source": [
        "#typical imports\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#other imports\n",
        "!pip install top2vec -q\n",
        "from top2vec import Top2Vec\n",
        "# import fasttext\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnMrJFRzEB7X"
      },
      "source": [
        "!pip uninstall numpy -q\n",
        "!pip install numpy==1.19.5 -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyGUE4zWDCS_"
      },
      "source": [
        "#train fastText\n",
        "\n",
        "#cbow model :\n",
        "ft_embeddings = fasttext.train_unsupervised('booking_sentences_processed.txt', model='cbow')\n",
        "\n",
        "# Skipgram model :\n",
        "# ft_embeddings = fasttext.train_unsupervised('fastText_training_data.txt', model='skipgram', verbose=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw8tCYrTDEO9"
      },
      "source": [
        "ft_embeddings.save_model('fast_text_embeddings.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbsEckNuDGec"
      },
      "source": [
        "#train doc2vec\n",
        "!wget https://raw.githubusercontent.com/akashjorss/topic_modelling/main/booking_sentences_processed.txt\n",
        "documents = []\n",
        "with open('booking_sentences_processed.txt', 'r') as f:\n",
        "    documents = f.readlines()\n",
        "# doc2vec = Top2Vec(documents, embedding_model='doc2vec', workers=8, document_ids=list(range(0,len(documents))), speed=\"deep-learn\", keep_documents=False)\n",
        "# doc2vec.save('doc2vec_top2vec.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpz1E3Q0DJaG"
      },
      "source": [
        "#Build model with USE\n",
        "use = Top2Vec(documents, embedding_model='universal-sentence-encoder', workers=8, document_ids=list(range(0,len(documents))), keep_documents=False)\n",
        "use.save('use_top2vec.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivj_M2Em0Bkd"
      },
      "source": [
        "#load sts dataset for evaluation\n",
        "# import pandas as pd\n",
        "# sts_df = pd.read_csv('sts-mt.csv')#, sep='\\t')\n",
        "# sts_df\n",
        "with open('sts-mt.csv', 'r') as f:\n",
        "  sts_lines = f.readlines() \n",
        "\n",
        "sts_df = []\n",
        "for l in sts_lines:\n",
        "  sts_df.append(l[:-1].split('\\t'))\n",
        "\n",
        "sentences1, sentences2, scores = [], [], []\n",
        "for l in sts_df:\n",
        "  scores.append(l[3])\n",
        "  sentences1.append(l[4])\n",
        "  sentences2.append(l[5])\n",
        "#scores are from 0 to 5. So the following line normalizes the score\n",
        "scores = [float(s)/5.0 for s in scores]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfCus9oFDLyv"
      },
      "source": [
        "#Build model with SBert\n",
        "from sentence_transformers import SentenceTransformer\n",
        "#download and save the sbert model\n",
        "sbert_model = SentenceTransformer('stsb-mpnet-base-v2')\n",
        "sbert.save(\"./stsb-mpnet-base-v2.pt\")\n",
        "#load and use SBert model\n",
        "sbert = Top2Vec(documents, embedding_model_path='./stsb-mpnet-base-v2.pt', workers=8, document_ids=list(range(0,len(documents))), keep_documents=False)\n",
        "sbert.save('sbert_top2vec.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj9syEZ-FGWO"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChTOgkuun5Yw"
      },
      "source": [
        "#Set up weights and biases to visualize the training of TSDAE\n",
        "# !pip install wandb -qqq\n",
        "# import wandb\n",
        "# wandb.login()\n",
        "wandb.init(\n",
        "  project='topic_modelling_booking_reviews',\n",
        "  config={\n",
        "      'pretrained_model':'bert-base-uncased',\n",
        "      'model': 'TSDAE',\n",
        "      'evaluation_metric': 'STS',\n",
        "      'dataset':'booking.com reviews (randomly sampled 100k)',\n",
        "      'weight_decay':0,\n",
        "      'scheduler':'constantlr',\n",
        "      'optimizer_params':{'lr': 3e-5},\n",
        "  }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45YxjBvD4Y00"
      },
      "source": [
        "#evaluator for Sentennce Transformer\n",
        "!pip install sentence_transformers -q\n",
        "from sentence_transformers import evaluation\n",
        "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WX7gB59DPnd"
      },
      "source": [
        "#train TSDAE\n",
        "!pip install sentence_transformers -q\n",
        "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
        "from sentence_transformers import models, util, datasets, evaluation, losses\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define your sentence transformer model using CLS pooling\n",
        "model_name = 'bert-base-uncased' #can we use stsb-mpnet-base-v2?\n",
        "word_embedding_model = models.Transformer(model_name)\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode_mean_tokens=False, pooling_mode_cls_token=True, pooling_mode_max_tokens=False)\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "# model.to('cuda')\n",
        "\n",
        "# Define a list with sentences (1k - 100k sentences)\n",
        "train_sentences = documents\n",
        "\n",
        "# Create the special denoising dataset that adds noise on-the-fly\n",
        "train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n",
        "# train_dataset.to('cuda')\n",
        "\n",
        "# DataLoader to batch your data\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Use the denoising auto-encoder loss\n",
        "train_loss = losses.DenoisingAutoEncoderLoss(model, decoder_name_or_path=model_name, tie_encoder_decoder=True)\n",
        "\n",
        "score_list = []\n",
        "steps_list = []\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#callback function\n",
        "def call_back(score, epoch, steps):\n",
        "  wandb.log({\"score\":score})\n",
        "\n",
        "# Call the fit method\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=1,\n",
        "    weight_decay=0,\n",
        "    scheduler='constantlr',\n",
        "    optimizer_params={'lr': 3e-5},\n",
        "    show_progress_bar=True,\n",
        "    save_best_model=True,\n",
        "    checkpoint_path = './tsdae_checkpoints/',\n",
        "    checkpoint_save_steps = 500, \n",
        "    checkpoint_save_total_limit = 2,\n",
        "    evaluation_steps=1,\n",
        "    callback=call_back,\n",
        "    evaluator=evaluator\n",
        "\n",
        ")\n",
        "model.save('tsdae-model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X83vWjc7r8Gi"
      },
      "source": [
        "wandb.logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5yPu5ZCEdnz"
      },
      "source": [
        "#build top2vec model using tsdae\n",
        "tsdae = Top2Vec(documents, embedding_model_path='./tsdae_model.pt', workers=8, document_ids=list(range(0,len(documents))), keep_documents=False)\n",
        "tsdae.save('sbert_top2vec.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ35exePlcRw"
      },
      "source": [
        "\"\"\"\n",
        "@article{wang-2021-TSDAE,\n",
        "    title = \"TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning\",\n",
        "    author = \"Wang, Kexin and Reimers, Nils and  Gurevych, Iryna\", \n",
        "    journal= \"arXiv preprint arXiv:2104.06979\",\n",
        "    month = \"4\",\n",
        "    year = \"2021\",\n",
        "    url = \"https://arxiv.org/abs/2104.06979\",\n",
        "}\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "colab": {
      "name": "train_models.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEn2wEqACaXj"
      },
      "source": [
        "#typical imports\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "#other imports\n",
        "# !pip install top2vec -q\n",
        "# from top2vec import Top2Vec\n",
        "# import fasttext\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnMrJFRzEB7X"
      },
      "source": [
        "!pip uninstall numpy -q\n",
        "!pip install numpy==1.19.5 -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyGUE4zWDCS_"
      },
      "source": [
        "#train fastText\n",
        "\n",
        "#cbow model :\n",
        "ft_embeddings = fasttext.train_unsupervised('booking_sentences_processed.txt', model='cbow')\n",
        "\n",
        "# Skipgram model :\n",
        "# ft_embeddings = fasttext.train_unsupervised('fastText_training_data.txt', model='skipgram', verbose=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw8tCYrTDEO9"
      },
      "source": [
        "ft_embeddings.save_model('fast_text_embeddings.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbsEckNuDGec"
      },
      "source": [
        "#train doc2vec\n",
        "!wget https://raw.githubusercontent.com/akashjorss/topic_modelling/main/booking_sentences_processed.txt\n",
        "documents = []\n",
        "with open('booking_sentences_processed.txt', 'r') as f:\n",
        "    documents = f.readlines()\n",
        "# doc2vec = Top2Vec(documents, embedding_model='doc2vec', workers=8, document_ids=list(range(0,len(documents))), speed=\"deep-learn\", keep_documents=False)\n",
        "# doc2vec.save('doc2vec_top2vec.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpz1E3Q0DJaG"
      },
      "source": [
        "#Build model with USE\n",
        "use = Top2Vec(documents, embedding_model='universal-sentence-encoder', workers=8, document_ids=list(range(0,len(documents))), keep_documents=False)\n",
        "use.save('use_top2vec.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfCus9oFDLyv"
      },
      "source": [
        "#Build model with SBert\n",
        "from sentence_transformers import SentenceTransformer\n",
        "#download and save the sbert model\n",
        "sbert_model = SentenceTransformer('stsb-mpnet-base-v2')\n",
        "sbert.save(\"./stsb-mpnet-base-v2.pt\")\n",
        "#load and use SBert model\n",
        "sbert = Top2Vec(documents, embedding_model_path='./stsb-mpnet-base-v2.pt', workers=8, document_ids=list(range(0,len(documents))), keep_documents=False)\n",
        "sbert.save('sbert_top2vec.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj9syEZ-FGWO"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChTOgkuun5Yw"
      },
      "source": [
        "#Set up weights and biases to visualize the training of TSDAE\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "  project='topic_modelling_booking_reviews',\n",
        "  config={\n",
        "      'pretrained_model':'bert-base-uncased',\n",
        "      'model': 'TSDAE',\n",
        "      'evaluation_metric': 'sts_processed',\n",
        "      'dataset':'booking.com reviews (randomly sampled 100k)',\n",
        "      'weight_decay':0,\n",
        "      'scheduler':'constantlr',\n",
        "      'optimizer_params':{'lr': 3e-5},\n",
        "  }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivj_M2Em0Bkd"
      },
      "source": [
        "#load processed sts data for evaluation\n",
        "with open('scores.txt') as f:\n",
        "  scores = f.readlines()\n",
        "scores = [float(s[:-1]) for s in scores]\n",
        "\n",
        "with open('sentences1.txt') as f:\n",
        "  sentences1 = f.readlines()\n",
        "sentences1 = [s[:-1] for s in sentences1]\n",
        "\n",
        "with open('sentences2.txt') as f:\n",
        "  sentences2 = f.readlines()\n",
        "sentences2 = [s[:-1] for s in sentences2]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WX7gB59DPnd"
      },
      "source": [
        "#train TSDAE\n",
        "!pip install sentence_transformers -q\n",
        "from sentence_transformers import SentenceTransformer, LoggingHandler\n",
        "from sentence_transformers import models, util, datasets, evaluation, losses\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sentence_transformers import evaluation\n",
        "evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1, sentences2, scores)\n",
        "\n",
        "# Define your sentence transformer model using CLS pooling\n",
        "model_name = 'bert-base-uncased' #can we use stsb-mpnet-base-v2?\n",
        "word_embedding_model = models.Transformer(model_name)\n",
        "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode_mean_tokens=False, pooling_mode_cls_token=True, pooling_mode_max_tokens=False)\n",
        "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
        "\n",
        "model.to('cuda')\n",
        "\n",
        "# Define a list with sentences (1k - 100k sentences)\n",
        "train_sentences = documents\n",
        "\n",
        "# Create the special denoising dataset that adds noise on-the-fly\n",
        "train_dataset = datasets.DenoisingAutoEncoderDataset(train_sentences)\n",
        "# train_dataset.to('cuda')\n",
        "\n",
        "# DataLoader to batch your data\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Use the denoising auto-encoder loss\n",
        "train_loss = losses.DenoisingAutoEncoderLoss(model, decoder_name_or_path=model_name, tie_encoder_decoder=True)\n",
        "\n",
        "score_list = []\n",
        "steps_list = []\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#callback function\n",
        "def call_back(score, epoch, steps):\n",
        "  wandb.log({\"score\":score, \"steps\":steps, \"epoch\": epoch, \"num_sentences\":steps*8, \"loss\":1-score})\n",
        "  print(\"steps:\", steps, \"score:\", score, \"loss:\", 1-score)\n",
        "\n",
        "# Call the fit method\n",
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    epochs=1,\n",
        "    weight_decay=0,\n",
        "    scheduler='constantlr',\n",
        "    optimizer_params={'lr': 3e-5},\n",
        "    show_progress_bar=True,\n",
        "    save_best_model=True,\n",
        "    checkpoint_path = './tsdae_checkpoints/',\n",
        "    checkpoint_save_steps = 500, \n",
        "    checkpoint_save_total_limit = 2,\n",
        "    evaluation_steps=100,\n",
        "    callback=call_back,\n",
        "    evaluator=evaluator\n",
        "\n",
        ")\n",
        "wandb.finish()\n",
        "# model.save('tsdae-model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYcQL5eUh-b-"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5yPu5ZCEdnz"
      },
      "source": [
        "#build top2vec model using tsdae\n",
        "tsdae = Top2Vec(documents, embedding_model_path='./tsdae.pt', workers=8, document_ids=list(range(0,len(documents))), keep_documents=False)\n",
        "tsdae.save('sbert_top2vec.bin')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Frzus8ZTg-Y9"
      },
      "source": [
        "import torch\n",
        "torch.save(model, \"tsdae.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ35exePlcRw"
      },
      "source": [
        "\"\"\"\n",
        "@article{wang-2021-TSDAE,\n",
        "    title = \"TSDAE: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning\",\n",
        "    author = \"Wang, Kexin and Reimers, Nils and  Gurevych, Iryna\", \n",
        "    journal= \"arXiv preprint arXiv:2104.06979\",\n",
        "    month = \"4\",\n",
        "    year = \"2021\",\n",
        "    url = \"https://arxiv.org/abs/2104.06979\",\n",
        "}\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Laxu2w3Cidha"
      },
      "source": [
        "\"\"\"The performance of TSDAE reduced over time on STS possibly because of \n",
        "we did lemm., stop word removal, etc. and the sentence structure doesn't match anymore. \n",
        "The only way I can think of right now is to apply the text processing steps to STS data set and then evaluate.\"\"\" "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
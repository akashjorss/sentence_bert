{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTYbySuqcYas"
      },
      "source": [
        "#install the packages\n",
        "!pip install sentence-transformers\n",
        "!pip install umap-learn\n",
        "# !pip install nltk\n",
        "# !pip install torch\n",
        "# !pip install tensorflow\n",
        "# !pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJLuNEtldf50"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorboard.plugins import projector\n",
        "#load tensorboards with magics\n",
        "%tensorflow_version 2.x\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_GradONc7zd"
      },
      "source": [
        "#import required packages\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GfMzjbhdM6_"
      },
      "source": [
        "!git clone https://github.com/akashjorss/sentence_bert\n",
        "!unzip sentence_bert/sentences.zip\n",
        "import pickle\n",
        "with open('sentences.bin', 'rb') as f:\n",
        "  sentences = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTb48ahddrF2"
      },
      "source": [
        "data = random.choices(sentences,k = 100000)\n",
        "# data = [s.lower() for s in data]\n",
        "\n",
        "# stop = stopwords.words('english')\n",
        "# sentiment_tags = ['ADJ', 'ADV', 'RB', 'VBZ']\n",
        "\n",
        "# for i in tqdm(range(len(data))):\n",
        "#   s = data[i]\n",
        "#   words = word_tokenize(s)\n",
        "#   #remove stop words\n",
        "#   words = [w for w in words if w not in stop]\n",
        "#   #remove sentiment words\n",
        "#   pos_tags = nltk.pos_tag(words)\n",
        "#   words = [t[0] for t in pos_tags if t[1] not in sentiment_tags]\n",
        "#   s = \" \".join(words)\n",
        "#   data[i] = s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmPEqk1QeKSC"
      },
      "source": [
        "#make sentence vectors\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens') #is it the correct sentence-bert?\n",
        "# model.to('cuda') -> not needed\n",
        "embeddings = model.encode(data, show_progress_bar=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB9_6EK4HmJy"
      },
      "source": [
        "#dim reduction using umap\n",
        "import umap\n",
        "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
        "                            n_components=3, \n",
        "                            metric='cosine').fit_transform(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdbKw81LePyO"
      },
      "source": [
        "#visualize high dim data in embedding projector\n",
        "# Set up a logs directory, so Tensorboard knows where to look for files\n",
        "import numpy as np\n",
        "log_dir='./logs/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# Save the weights we want to analyse as a variable. \n",
        "umap_weights = tf.Variable(random.choices(umap_embeddings, k=10000))\n",
        "# Create a checkpoint from embedding, the filename and key are\n",
        "# name of the tensor.\n",
        "checkpoint = tf.train.Checkpoint(embedding=umap_weights)\n",
        "checkpoint.save(os.path.join(log_dir, \"umap_embedding.ckpt\"))\n",
        "\n",
        "# Set up config\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`\n",
        "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
        "# embedding.metadata_path = 'metadata.tsv'\n",
        "projector.visualize_embeddings(log_dir, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SbEz5MmeVE0"
      },
      "source": [
        "%tensorboard --logdir ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHpVrufth05I"
      },
      "source": [
        "#perform dim reduction hdbscan and get the topics\n",
        "#Do we need to reduce the dimensionality? Can't clustering work in high dim data?\n",
        "import umap\n",
        "umap_embeddings = umap.UMAP(n_neighbors=15, \n",
        "                            n_components=5, \n",
        "                            metric='cosine').fit_transform(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppKjpjpLnmTQ"
      },
      "source": [
        "!pip install hdbscan\n",
        "import hdbscan\n",
        "cluster = hdbscan.HDBSCAN(min_cluster_size=20,\n",
        "                          metric='euclidean',  #can use diff method here                    \n",
        "                          cluster_selection_method='eom').fit(umap_embeddings) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFbC49Ywn9oJ"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import umap\n",
        "# Prepare data\n",
        "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
        "# pca_data = PCA(n_components=2).fit_transform(embeddings)\n",
        "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
        "result['labels'] = cluster.labels_\n",
        "\n",
        "# Visualize clusters\n",
        "fig, ax = plt.subplots(figsize=(20, 10))\n",
        "outliers = result.loc[result.labels == -1, :]\n",
        "clustered = result.loc[result.labels != -1, :]\n",
        "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
        "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
        "plt.colorbar()\n",
        "\n",
        "docs_df = pd.DataFrame(data, columns=[\"Doc\"])\n",
        "docs_df['Topic'] = cluster.labels_\n",
        "docs_df['Doc_ID'] = range(len(docs_df))\n",
        "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
        "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
        "    t = count.transform(documents).toarray()\n",
        "    w = t.sum(axis=1)\n",
        "    tf = np.divide(t.T, w)\n",
        "    sum_t = t.sum(axis=0)\n",
        "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
        "    tf_idf = np.multiply(tf, idf)\n",
        "\n",
        "    return tf_idf, count\n",
        "  \n",
        "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(data))\n",
        "\n",
        "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20):\n",
        "    words = count.get_feature_names()\n",
        "    labels = list(docs_per_topic.Topic)\n",
        "    tf_idf_transposed = tf_idf.T\n",
        "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
        "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
        "    return top_n_words\n",
        "\n",
        "def extract_topic_sizes(df):\n",
        "    topic_sizes = (df.groupby(['Topic'])\n",
        "                     .Doc\n",
        "                     .count()\n",
        "                     .reset_index()\n",
        "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
        "                     .sort_values(\"Size\", ascending=False))\n",
        "    return topic_sizes\n",
        "\n",
        "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
        "topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)\n",
        "\n",
        "top_n_words\n",
        "\n",
        "# from sklearn.metrics.pairwise import cosine_similarity\n",
        "# for i in range(20):\n",
        "#     # Calculate cosine similarity\n",
        "#     similarities = cosine_similarity(tf_idf.T)\n",
        "#     np.fill_diagonal(similarities, 0)\n",
        "\n",
        "#     # Extract label to merge into and from where\n",
        "#     topic_sizes = docs_df.groupby(['Topic']).count().sort_values(\"Doc\", ascending=False).reset_index()\n",
        "#     topic_to_merge = topic_sizes.iloc[-1].Topic\n",
        "#     topic_to_merge_into = np.argmax(similarities[topic_to_merge + 1]) - 1\n",
        "\n",
        "#     # Adjust topics\n",
        "#     docs_df.loc[docs_df.Topic == topic_to_merge, \"Topic\"] = topic_to_merge_into\n",
        "#     old_topics = docs_df.sort_values(\"Topic\").Topic.unique()\n",
        "#     map_topics = {old_topic: index - 1 for index, old_topic in enumerate(old_topics)}\n",
        "#     docs_df.Topic = docs_df.Topic.map(map_topics)\n",
        "#     docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
        "\n",
        "#     # Calculate new topic words\n",
        "#     m = len(data)\n",
        "#     tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m)\n",
        "#     top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, n=20)\n",
        "\n",
        "# topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYcyRu5BoFKb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}